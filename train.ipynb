{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for IANNwTF 2022/23 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to colorize grayscale dog pictures with the Stanford Dog Dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:06:26.982038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 15:06:27.353817: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-23 15:06:27.523062: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-23 15:06:28.258279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lotta/apps/anaconda3/envs/tf-gpu/lib/\n",
      "2023-02-23 15:06:28.258395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lotta/apps/anaconda3/envs/tf-gpu/lib/\n",
      "2023-02-23 15:06:28.258398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage.io import imread, imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Conv2D, Reshape, GlobalAveragePooling2D, MaxPooling2D, UpSampling2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:06:29.471482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:29.513133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:29.513483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:29.518636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 15:06:29.521881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:29.522202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:29.522381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:30.312809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:30.313446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:30.313501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 15:06:30.313553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21984 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "# makes images same size and fills gaps at the edges with black pixels\n",
    "\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "    # Check tha amount of padding needed to be done.\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "\n",
    "    # Only necessary if you want to do same amount of padding on both sides.\n",
    "    if pad_height % 2 != 0:\n",
    "        height = pad_height // 2\n",
    "        pad_height_top = height + 1\n",
    "        pad_height_bottom = height\n",
    "    else:\n",
    "        pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "    if pad_width % 2 != 0:\n",
    "        width = pad_width // 2\n",
    "        pad_width_left = width + 1\n",
    "        pad_width_right = width\n",
    "    else:\n",
    "        pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "    image = tf.pad(\n",
    "        image,\n",
    "        paddings=[\n",
    "            [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    #image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    # go through folders \n",
    "    # make pairs of images + breed\n",
    "    # (not needed for grayscale but might need it later)\n",
    "    # divide into test and train\n",
    "    base_path = \"data/Images\"\n",
    "    lookup_table_breeds = {}\n",
    "    train_img = []\n",
    "    train_lbl = []\n",
    "    test_img = []\n",
    "    test_lbl = []\n",
    "    for num,folder in enumerate(os.listdir(base_path)):\n",
    "        lookup_table_breeds[folder[10:]] = num\n",
    "        image_paths = os.path.join(base_path, folder)\n",
    "        for count, image_path in enumerate(os.listdir(image_paths)):\n",
    "            path = os.path.join(image_paths, image_path)\n",
    "            if 0.9 * len(list(folder)) < count:\n",
    "                # makes images same size and fills gaps at the edges with black pixels\n",
    "                image = distortion_free_resize(tf.image.decode_jpeg(tf.io.read_file(path),3), (128,128))\n",
    "                # convert into Lab color space\n",
    "                train_img.append(rgb2lab(image/255))\n",
    "                train_lbl.append(lookup_table_breeds[folder[10:]])\n",
    "\n",
    "            else:\n",
    "                # makes images same size and fills gaps at the edges with black pixels\n",
    "                image = distortion_free_resize(tf.image.decode_jpeg(tf.io.read_file(path),3), (128,128))\n",
    "                # convert into Lab color space\n",
    "                test_img.append(rgb2lab(image/255))            \n",
    "                test_lbl.append(lookup_table_breeds[folder[10:]])\n",
    "\n",
    "    train_images = tf.data.Dataset.from_tensor_slices(train_img)\n",
    "    tf.data.Dataset.save(train_images, \"saved_datasets/train_images\")\n",
    "    print(train_images)\n",
    "    train_labels = tf.data.Dataset.from_tensor_slices(train_lbl)\n",
    "    tf.data.Dataset.save(train_labels, \"saved_datasets/train_labels\")\n",
    "    print(train_labels)\n",
    "\n",
    "    test_images = tf.data.Dataset.from_tensor_slices(test_img)\n",
    "    tf.data.Dataset.save(test_images, \"saved_datasets/test_images\")\n",
    "    print(test_images)\n",
    "    test_labels = tf.data.Dataset.from_tensor_slices(test_lbl)\n",
    "    tf.data.Dataset.save(test_labels, \"saved_datasets/test_labels\")\n",
    "    print(test_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    train_images = tf.data.Dataset.load(\"saved_datasets/train_images\")\n",
    "    train_labels = tf.data.Dataset.load(\"saved_datasets/train_labels\")\n",
    "    test_images = tf.data.Dataset.load(\"saved_datasets/test_images\")\n",
    "    test_labels = tf.data.Dataset.load(\"saved_datasets/test_labels\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "datasets_stored = True\n",
    "\n",
    "if datasets_stored:\n",
    "    train_images, train_labels, test_images, test_labels = load_datasets()\n",
    "else:\n",
    "    train_images, train_labels, test_images, test_labels = prepare_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 120), dtype=tf.int16, name=None))>\n",
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 120), dtype=tf.int16, name=None))>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def preprocess_dataset(images, labels):\n",
    "    \n",
    "    # flip each image left-right with a chance of 0.3\n",
    "    images = images.map(lambda x: (tf.reverse(x, axis=[-2])) if random.random() < 0.5 else (x))\n",
    "    images = images.map(lambda x: (tf.reverse(x, axis=[-3])) if random.random() < 0.5 else (x))\n",
    "\n",
    "    # divide into greyscale input and color output\n",
    "\n",
    "    images = images.map(lambda x: ((tf.expand_dims(x[:,:,0], -1))/100, (x[:,:,1:]/128)))\n",
    "    labels = labels.map(lambda x: tf.one_hot(x, 120))\n",
    "    labels = labels.map(lambda x: (tf.cast(x, tf.int16)))\n",
    "\n",
    "    zipped = tf.data.Dataset.zip((images, labels))\n",
    "    \n",
    "    zipped = zipped.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return zipped\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = preprocess_dataset(train_images, train_labels)\n",
    "test_dataset = preprocess_dataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "# the dataset has the format\n",
    "# greyscale images (64,64), a and b terms from lab color space (64,64,2), onehotted labels (120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# or take different crops from the pictures\n",
    "\n",
    "# show sample pictures from dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Low_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv2 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(128, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv4 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv5 = Conv2D(256, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv6 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mid_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv2 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class High_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(512, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv2 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(512, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv4 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(1024, activation=\"relu\")\n",
    "        self.dense2 = Dense(512, activation=\"relu\")\n",
    "        self.dense3 = Dense(256, activation=\"relu\")\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Network(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(256, activation=\"relu\")\n",
    "        self.dense2 = Dense(120, activation=\"softmax\")\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Layer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #32,256,256\n",
    "        #256\n",
    "        self.dense1 = Dense(256, activation=\"relu\")\n",
    "        self.conv = Conv2D(256,1, activation=\"relu\", padding=\"same\", strides=1)\n",
    "\n",
    "    def __call__(self, mid_level, global_vector, training=False):\n",
    "        #x = tf.concat((mid_level, global_vector), axis=-1)\n",
    "        #print(x)\n",
    "        #x = self.dense1(x) \n",
    "        global_vector = tf.expand_dims(tf.expand_dims(global_vector,axis=1),axis=1)\n",
    "        print(mid_level, global_vector)\n",
    "\n",
    "        x = self.conv(tf.concat((mid_level, global_vector), axis = -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization_Network(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "        self.upsampling1 = UpSampling2D(2)\n",
    "        self.conv2 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(64, 3, activation='relu', padding='same', strides=1)\n",
    "        self.upsampling2 = UpSampling2D(2) \n",
    "        self.conv4 = Conv2D(32, 3, activation='relu', padding='same', strides=1)\n",
    "        self.upsampling3 = UpSampling2D(2) \n",
    "        self.conv5 = Conv2D(2, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.conv1(x)\n",
    "        x = self.upsampling1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.upsampling2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.upsampling3(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization_Model(tf.keras.Model):\n",
    "    def __init__(self, optimizer, loss_function_color, loss_function_category):\n",
    "        super().__init__()\n",
    "        self.low_level = Low_Level_Features()        \n",
    "        self.mid_level = Mid_Level_Features()\n",
    "        self.high_level = High_Level_Features()\n",
    "        self.fusion = Fusion_Layer()\n",
    "        self.colorization = Colorization_Network()\n",
    "        self.classification = Classification_Network()\n",
    "\n",
    "        self.metrics_list = [\n",
    "            tf.keras.metrics.Mean(name=\"loss_color\"),\n",
    "            tf.keras.metrics.Mean(name=\"loss_category\")]\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function_color = loss_function_color\n",
    "        self.loss_function_category = loss_function_category\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        low = self.low_level(input)\n",
    "        middle = self.mid_level(low)\n",
    "        high = self.high_level(low)\n",
    "        fused = self.fusion(middle, high)\n",
    "        colored = self.colorization(fused)\n",
    "        label = self.classification(high)\n",
    "        return colored, label\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        images,  label = data\n",
    "        grey_image, color_image = images\n",
    "        with tf.GradientTape() as tape: \n",
    "            predicted_color, predicted_label = self(grey_image, training = True)\n",
    "            loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "            loss_category  = self.loss_function_category(label, predicted_label)\n",
    "\n",
    "        gradients = tape.gradient([loss_color, loss_category], self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        self.metrics[1].update_state(loss_category)  \n",
    "        return gradients\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        images, label = data\n",
    "        grey_image, color_image = images    \n",
    "        predicted_color, predicted_label = self(grey_image, training = True)\n",
    "        loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "        loss_category  = self.loss_function_category(label, predicted_label)            \n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        self.metrics[1].update_state(loss_category)  \n",
    "        return predicted_color, color_image, predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder from https://arxiv.org/pdf/1712.03400.pdf\n",
    "\n",
    "# model\n",
    "\n",
    "# create the whole autoencoder model\n",
    "# (steal from https://towardsdatascience.com/image-colorization-using-convolutional-autoencoders-fdabc1cb1dbe )\n",
    "\n",
    "#encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    #input 1,128,128\n",
    "    self.conv1 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv2 = Conv2D(128, 3, activation='relu', padding='same', strides=2) \n",
    "    self.conv3 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv4 = Conv2D(256, 3, activation='relu', padding='same', strides=2) \n",
    "    self.conv5 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv6 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv7 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv8 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x, training=False):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.conv6(x)\n",
    "    x = self.conv7(x)\n",
    "    x = self.conv8(x)\n",
    "    x = self.flatten(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reshape = Reshape((32, 32, 256))\n",
    "        \n",
    "        self.conv1 = Conv2D(256, 3, activation=\"relu\", padding=\"same\", strides=1)\n",
    "        self.conv2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.upsampling2 = UpSampling2D(2)\n",
    "        self.conv3 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.conv4 = Conv2D(64, 3, activation=\"tanh\", padding=\"same\")\n",
    "        self.upsampling4 = UpSampling2D(2)\n",
    "        self.conv5 = Conv2D(32, 3, activation=\"tanh\", padding=\"same\")\n",
    "        self.conv5 = Conv2D(2, 3, activation=\"tanh\", padding=\"same\")\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsampling2(x)        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)        \n",
    "        x = self.upsampling4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, optimizer, loss_function):\n",
    "    super().__init__()\n",
    "    self.enc = Encoder()\n",
    "    self.dec = Decoder()\n",
    "\n",
    "    self.metrics_list = [\n",
    "      tf.keras.metrics.Mean(name=\"loss\")]\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return self.metrics_list\n",
    "  \n",
    "  def get_encoder(self):\n",
    "    return self.enc\n",
    "   \n",
    "  def get_decoder(self):\n",
    "    return self.dec\n",
    "    \n",
    "  def reset_metrics(self):\n",
    "     for metric in self.metrics:\n",
    "        metric.reset_state()\n",
    "\n",
    "  def call(self, input, training=False):\n",
    "    embedding = self.enc(input)\n",
    "    output = self.dec(embedding)\n",
    "    return output\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    images,  label = data\n",
    "    grey_image, color_image = images\n",
    "    with tf.GradientTape() as tape: \n",
    "      prediction = self(grey_image, training = True)\n",
    "      loss = self.loss_function(color_image, prediction)\n",
    "\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "    self.metrics[0].update_state(loss)  \n",
    "    return gradients\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "    images, label = data\n",
    "    grey_image, color_image = images    \n",
    "    prediction = self(grey_image, training = False)\n",
    "    loss = self.loss_function(color_image, prediction)\n",
    "    self.metrics[0].update_state(loss)\n",
    "    return prediction, color_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# log results with tensorboard \n",
    "# save model to be able to reuse it\n",
    "\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            model.train_step(data)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        print(\"Loss Category: \", model.metrics[1].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in tqdm(test_ds, position=0, leave=True):\n",
    "            prediction, target = model.test_step(data)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "            \n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "        print(\"Loss Category: \", model.metrics[1].result().numpy(), \"(Test)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/282 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"colorization__model_6/conv2d_228/Relu:0\", shape=(64, 32, 32, 256), dtype=float32) Tensor(\"colorization__model_6/ExpandDims_1:0\", shape=(64, 1, 1, 256), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_5121/1995622918.py\", line 41, in train_step  *\n        predicted_color, predicted_label = self(grey_image, training = True)\n    File \"/home/lotta/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebhopfgqs.py\", line 13, in tf__call\n        fused = ag__.converted_call(ag__.ld(self).fusion, (ag__.ld(middle), ag__.ld(high)), None, fscope)\n    File \"/tmp/__autograph_generated_file5x0viw1p.py\", line 12, in tf____call__\n        x = ag__.converted_call(ag__.ld(self).conv, (ag__.converted_call(ag__.ld(tf).concat, ((ag__.ld(mid_level), ag__.ld(global_vector)),), dict(axis=-1), fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"colorization__model_6\" \"                 f\"(type Colorization_Model).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_5121/1995622918.py\", line 31, in call  *\n            fused = self.fusion(middle, high)\n        File \"/tmp/ipykernel_5121/1992193503.py\", line 16, in __call__  *\n            x = self.conv(tf.concat((mid_level, global_vector), axis = -1))\n    \n        ValueError: Dimension 1 in both shapes must be equal, but are 32 and 1. Shapes are [64,32,32] and [64,1,1]. for '{{node colorization__model_6/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](colorization__model_6/conv2d_228/Relu, colorization__model_6/ExpandDims_1, colorization__model_6/concat/axis)' with input shapes: [64,32,32,256], [64,1,1,256], [] and with computed input tensors: input[2] = <-1>.\n    \n    \n    Call arguments received by layer \"colorization__model_6\" \"                 f\"(type Colorization_Model):\n      • input=tf.Tensor(shape=(64, 128, 128, 1), dtype=float32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m train_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(train_log_path)\n\u001b[1;32m     16\u001b[0m test_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(test_log_path)\n\u001b[0;32m---> 17\u001b[0m training_loop(colorization_model, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)\n",
      "Cell \u001b[0;32mIn[136], line 11\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     14\u001b[0m     tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname, model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filepf34_yzc.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m (grey_image, color_image) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(images)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 13\u001b[0m     (predicted_color, predicted_label) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(grey_image),), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)\n\u001b[1;32m     14\u001b[0m     loss_color \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mloss_function_color, (ag__\u001b[39m.\u001b[39mld(color_image), ag__\u001b[39m.\u001b[39mld(predicted_color)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m     loss_category \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mloss_function_category, (ag__\u001b[39m.\u001b[39mld(label), ag__\u001b[39m.\u001b[39mld(predicted_label)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebhopfgqs.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     11\u001b[0m middle \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmid_level, (ag__\u001b[39m.\u001b[39mld(low),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m high \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mhigh_level, (ag__\u001b[39m.\u001b[39mld(low),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 13\u001b[0m fused \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfusion, (ag__\u001b[39m.\u001b[39;49mld(middle), ag__\u001b[39m.\u001b[39;49mld(high)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m colored \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcolorization, (ag__\u001b[39m.\u001b[39mld(fused),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m label \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mclassification, (ag__\u001b[39m.\u001b[39mld(high),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5x0viw1p.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, mid_level, global_vector, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m global_vector \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mexpand_dims, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mexpand_dims, (ag__\u001b[39m.\u001b[39mld(global_vector),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope)\n\u001b[1;32m     11\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(mid_level), ag__\u001b[39m.\u001b[39mld(global_vector))\n\u001b[0;32m---> 12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mconcat, ((ag__\u001b[39m.\u001b[39;49mld(mid_level), ag__\u001b[39m.\u001b[39;49mld(global_vector)),), \u001b[39mdict\u001b[39;49m(axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_5121/1995622918.py\", line 41, in train_step  *\n        predicted_color, predicted_label = self(grey_image, training = True)\n    File \"/home/lotta/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebhopfgqs.py\", line 13, in tf__call\n        fused = ag__.converted_call(ag__.ld(self).fusion, (ag__.ld(middle), ag__.ld(high)), None, fscope)\n    File \"/tmp/__autograph_generated_file5x0viw1p.py\", line 12, in tf____call__\n        x = ag__.converted_call(ag__.ld(self).conv, (ag__.converted_call(ag__.ld(tf).concat, ((ag__.ld(mid_level), ag__.ld(global_vector)),), dict(axis=-1), fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"colorization__model_6\" \"                 f\"(type Colorization_Model).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_5121/1995622918.py\", line 31, in call  *\n            fused = self.fusion(middle, high)\n        File \"/tmp/ipykernel_5121/1992193503.py\", line 16, in __call__  *\n            x = self.conv(tf.concat((mid_level, global_vector), axis = -1))\n    \n        ValueError: Dimension 1 in both shapes must be equal, but are 32 and 1. Shapes are [64,32,32] and [64,1,1]. for '{{node colorization__model_6/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](colorization__model_6/conv2d_228/Relu, colorization__model_6/ExpandDims_1, colorization__model_6/concat/axis)' with input shapes: [64,32,32,256], [64,1,1,256], [] and with computed input tensors: input[2] = <-1>.\n    \n    \n    Call arguments received by layer \"colorization__model_6\" \"                 f\"(type Colorization_Model):\n      • input=tf.Tensor(shape=(64, 128, 128, 1), dtype=float32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "epochs = 30\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_function_color = tf.keras.losses.MeanSquaredError()\n",
    "loss_function_category = tf.keras.losses.CategoricalCrossentropy()\n",
    "#autoencoder = Autoencoder(optimizer=optimizer, loss_function=loss_function)\n",
    "\n",
    "colorization_model= Colorization_Model(optimizer=optimizer, loss_function_color=loss_function_color, loss_function_category=loss_function_category)\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"models/{current_time}\"\n",
    "train_log_path = f\"logs/{current_time}/train\"\n",
    "test_log_path = f\"logs/{current_time}/test\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "training_loop(colorization_model, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-81ffcf342eae6aba\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-81ffcf342eae6aba\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be5498235a11125f8b9cbeecb800fcd2f42e8279b58f495bdc82a203ff5bcb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
