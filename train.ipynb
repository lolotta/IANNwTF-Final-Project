{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for IANNwTF 2022/23 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to colorize grayscale dog pictures with the Stanford Dog Dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage.io import imread, imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.layers import Dense, Conv2D, Reshape, GlobalAveragePooling2D, MaxPooling2D, UpSampling2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "# makes images same size and fills gaps at the edges with black pixels\n",
    "\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "    # Check tha amount of padding needed to be done.\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "\n",
    "    # Only necessary if you want to do same amount of padding on both sides.\n",
    "    if pad_height % 2 != 0:\n",
    "        height = pad_height // 2\n",
    "        pad_height_top = height + 1\n",
    "        pad_height_bottom = height\n",
    "    else:\n",
    "        pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "    if pad_width % 2 != 0:\n",
    "        width = pad_width // 2\n",
    "        pad_width_left = width + 1\n",
    "        pad_width_right = width\n",
    "    else:\n",
    "        pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "    image = tf.pad(\n",
    "        image,\n",
    "        paddings=[\n",
    "            [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    #image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    # go through folders \n",
    "    # make pairs of images + breed\n",
    "    # (not needed for grayscale but might need it later)\n",
    "    # divide into test and train\n",
    "    base_path = \"data/Images\"\n",
    "    lookup_table_breeds = {}\n",
    "    inverse_lookup_table = {}\n",
    "    train_img = []\n",
    "    train_lbl = []\n",
    "    test_img = []\n",
    "    test_lbl = []\n",
    "    for num,folder in enumerate(os.listdir(base_path)):\n",
    "        lookup_table_breeds[folder[10:]] = num\n",
    "        image_paths = os.path.join(base_path, folder)\n",
    "        for count, image_path in enumerate(os.listdir(image_paths)):\n",
    "            path = os.path.join(image_paths, image_path)\n",
    "            if 0.9 * len(list(folder)) < count:\n",
    "                # makes images same size and fills gaps at the edges with black pixels\n",
    "                image = distortion_free_resize(tf.image.decode_jpeg(tf.io.read_file(path),3), (128,128))\n",
    "                # convert into Lab color space\n",
    "                train_img.append(rgb2lab(image/255))\n",
    "                train_lbl.append(lookup_table_breeds[folder[10:]])\n",
    "\n",
    "            else:\n",
    "                # makes images same size and fills gaps at the edges with black pixels\n",
    "                image = distortion_free_resize(tf.image.decode_jpeg(tf.io.read_file(path),3), (128,128))\n",
    "                # convert into Lab color space\n",
    "                test_img.append(rgb2lab(image/255))            \n",
    "                test_lbl.append(lookup_table_breeds[folder[10:]])\n",
    "\n",
    "    inverse_lookup_table = {v: k for k, v in lookup_table_breeds.items()}\n",
    "    with open('saved_lookup_table.pkl', 'wb') as f:\n",
    "        pickle.dump(inverse_lookup_table, f)\n",
    "\n",
    "    train_images = tf.data.Dataset.from_tensor_slices(train_img)\n",
    "    tf.data.Dataset.save(train_images, \"saved_datasets/train_images\")\n",
    "    print(train_images)\n",
    "    train_labels = tf.data.Dataset.from_tensor_slices(train_lbl)\n",
    "    tf.data.Dataset.save(train_labels, \"saved_datasets/train_labels\")\n",
    "    print(train_labels)\n",
    "\n",
    "    test_images = tf.data.Dataset.from_tensor_slices(test_img)\n",
    "    tf.data.Dataset.save(test_images, \"saved_datasets/test_images\")\n",
    "    print(test_images)\n",
    "    test_labels = tf.data.Dataset.from_tensor_slices(test_lbl)\n",
    "    tf.data.Dataset.save(test_labels, \"saved_datasets/test_labels\")\n",
    "    print(test_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels, inverse_lookup_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    train_images = tf.data.Dataset.load(\"saved_datasets/train_images\")\n",
    "    train_labels = tf.data.Dataset.load(\"saved_datasets/train_labels\")\n",
    "    test_images = tf.data.Dataset.load(\"saved_datasets/test_images\")\n",
    "    test_labels = tf.data.Dataset.load(\"saved_datasets/test_labels\")\n",
    "\n",
    "    with open('saved_lookup_table.pkl', 'rb') as f:\n",
    "        inverse_lookup_table = pickle.load(f)\n",
    "\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels, inverse_lookup_table\n",
    "\n",
    "datasets_stored = True\n",
    "\n",
    "if datasets_stored:\n",
    "    train_images, train_labels, test_images, test_labels, inverse_lookup_table = load_datasets()\n",
    "else:\n",
    "    train_images, train_labels, test_images, test_labels, inverse_lookup_table = prepare_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 120), dtype=tf.int16, name=None))>\n",
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 120), dtype=tf.int16, name=None))>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def preprocess_dataset(images, labels):\n",
    "    \n",
    "    # flip each image left-right with a chance of 0.5\n",
    "    images = images.map(lambda x: (tf.reverse(x, axis=[-2])) if random.random() < 0.5 else (x))\n",
    "    images = images.map(lambda x: (tf.reverse(x, axis=[-3])) if random.random() < 0.5 else (x))\n",
    "\n",
    "    # divide into greyscale input and color output\n",
    "\n",
    "    images = images.map(lambda x: ((tf.expand_dims(x[:,:,0], -1))/100, (x[:,:,1:]/128)))\n",
    "    labels = labels.map(lambda x: tf.one_hot(x, 120))\n",
    "    labels = labels.map(lambda x: (tf.cast(x, tf.int16)))\n",
    "\n",
    "    \n",
    "    # or zip first and then do calculations??\n",
    "    zipped = tf.data.Dataset.zip((images, labels))\n",
    "    \n",
    "    zipped = zipped.cache().shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return zipped\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = preprocess_dataset(train_images, train_labels)\n",
    "test_dataset = preprocess_dataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "# the dataset has the format\n",
    "# greyscale images (64,64), a and b terms from lab color space (64,64,2), onehotted labels (120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# or take different crops from the pictures\n",
    "\n",
    "# show sample pictures from dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Low_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv2 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(128, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv4 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv5 = Conv2D(256, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv6 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mid_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv2 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "class High_Level_Features(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(512, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv2 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(512, 3, activation='relu', padding='same', strides=2) \n",
    "        self.conv4 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(1024, activation=\"relu\")\n",
    "        self.dense2 = Dense(512, activation=\"relu\")\n",
    "        self.dense3 = Dense(256, activation=\"relu\")\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Network(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(256, activation=\"relu\")\n",
    "        self.dense2 = Dense(120, activation=\"softmax\")\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Layer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #32,256,256\n",
    "        #\n",
    "\n",
    "        self.repeat_layer = tf.keras.layers.RepeatVector(32*32)\n",
    "        self.reshape = tf.keras.layers.Reshape(([32,32,256]))\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=3)\n",
    "        self.conv = Conv2D(256, kernel_size=1,strides=1, activation=\"relu\", padding=\"same\")\n",
    "\n",
    "\n",
    "    def __call__(self, mid_level, global_vector, training=False):\n",
    "        x = self.repeat_layer(global_vector) \n",
    "        x = self.reshape(x)\n",
    "        x = self.concat([mid_level, x]) \n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization_Network(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "        self.upsampling1 = UpSampling2D(2)\n",
    "        self.conv2 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "        self.conv3 = Conv2D(64, 3, activation='relu', padding='same', strides=1)\n",
    "        self.upsampling2 = UpSampling2D(2) \n",
    "        self.conv4 = Conv2D(32, 3, activation='relu', padding='same', strides=1)\n",
    "        self.conv5 = Conv2D(2, 3, activation='tanh', padding='same', strides=1) \n",
    "\n",
    "    def __call__(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.upsampling1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.upsampling2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorization_Model(tf.keras.Model):\n",
    "    def __init__(self, low_level):\n",
    "        super().__init__()\n",
    "        self.low_level = low_level\n",
    "        self.mid_level = Mid_Level_Features()\n",
    "        self.fusion = Fusion_Layer()\n",
    "        self.colorization = Colorization_Network()\n",
    "\n",
    "\n",
    "    def call(self, input, high_level_input, training=False):\n",
    "        low = self.low_level(input)\n",
    "        middle = self.mid_level(low)\n",
    "        fused = self.fusion(middle, high_level_input)\n",
    "        colored = self.colorization(fused)\n",
    "        return colored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Model(tf.keras.Model):\n",
    "    def __init__(self, low_level):\n",
    "        super().__init__()\n",
    "        self.low_level = low_level \n",
    "        self.high_level = High_Level_Features()\n",
    "        self.classification = Classification_Network()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        low = self.low_level(input)\n",
    "        high = self.high_level(low)\n",
    "        label = self.classification(high)\n",
    "        return high, label\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Only_Colorization_Model(tf.keras.Model):\n",
    "    def __init__(self, optimizer, loss_function_color):\n",
    "        super().__init__()\n",
    "        self.low_level = Low_Level_Features()\n",
    "        self.mid_level = Mid_Level_Features()\n",
    "        self.colorization = Colorization_Network()\n",
    "\n",
    "\n",
    "        self.metrics_list = [\n",
    "            tf.keras.metrics.Mean(name=\"loss_color\"),\n",
    "        ]\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function_color = loss_function_color\n",
    "\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        low = self.low_level(input)\n",
    "        middle = self.mid_level(low, training=training)\n",
    "        colored = self.colorization(middle)\n",
    "        return colored\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        images,  label = data\n",
    "        grey_image, color_image = images\n",
    "        with tf.GradientTape() as color_tape: \n",
    "            predicted_color = self(grey_image, training = True)\n",
    "            loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "\n",
    "        gradients_color = color_tape.gradient(loss_color, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients_color, self.trainable_variables))\n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        return predicted_color\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        images, label = data\n",
    "        grey_image, color_image = images    \n",
    "        predicted_color = self(grey_image, training = False)\n",
    "        loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        return predicted_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Only_Classification_Model(tf.keras.Model):\n",
    "    def __init__(self, optimizer, loss_function_category):\n",
    "        super().__init__()\n",
    "        self.low_level = Low_Level_Features()        \n",
    "        self.classification_model = Classification_Model(self.low_level)\n",
    "\n",
    "        self.metrics_list = [\n",
    "            tf.keras.metrics.Mean(name=\"loss_category\")]\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function_category = loss_function_category\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        _, label = self.classification_model(input)\n",
    "        return label\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        images,  label = data\n",
    "        grey_image, color_image = images\n",
    "        with tf.GradientTape() as class_tape: \n",
    "            predicted_label = self(grey_image, training = True)\n",
    "            loss_category = self.loss_function_category(label, predicted_label)\n",
    "\n",
    "        gradients_category = class_tape.gradient(loss_category, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients_category, self.trainable_variables))\n",
    "        self.metrics[0].update_state(loss_category)  \n",
    "        return predicted_label\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        images, label = data\n",
    "        grey_image, color_image = images    \n",
    "        predicted_label = self(grey_image, training = False)\n",
    "        loss_category  = self.loss_function_category(label, predicted_label)            \n",
    "        self.metrics[0].update_state(loss_category)  \n",
    "        return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, optimizer, loss_function_color, loss_function_category):\n",
    "        super().__init__()\n",
    "        self.low_level = Low_Level_Features()        \n",
    "\n",
    "        self.colorization_model = Colorization_Model(self.low_level)\n",
    "        self.classification_model = Classification_Model(self.low_level)\n",
    "\n",
    "        self.metrics_list = [\n",
    "            tf.keras.metrics.Mean(name=\"loss_color\"),\n",
    "            tf.keras.metrics.Mean(name=\"loss_category\")]\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function_color = loss_function_color\n",
    "        self.loss_function_category = loss_function_category\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        high_level_info, label = self.classification_model(input)\n",
    "        colored = self.colorization_model(input, high_level_info)\n",
    "        return colored, label\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        images,  label = data\n",
    "        grey_image, color_image = images\n",
    "        with tf.GradientTape() as color_tape, tf.GradientTape() as class_tape: \n",
    "            predicted_color, predicted_label = self(grey_image, training = True)\n",
    "            loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "            loss_category = self.loss_function_category(label, predicted_label)\n",
    "\n",
    "        gradients_color = color_tape.gradient(loss_color, self.colorization_model.trainable_variables)\n",
    "        gradients_category = class_tape.gradient(loss_category, self.classification_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients_color, self.colorization_model.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients_category, self.classification_model.trainable_variables))\n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        self.metrics[1].update_state(loss_category)  \n",
    "        return predicted_color, predicted_label\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        images, label = data\n",
    "        grey_image, color_image = images    \n",
    "        predicted_color, predicted_label = self(grey_image, training = False)\n",
    "        loss_color = self.loss_function_color(color_image, predicted_color)\n",
    "        loss_category  = self.loss_function_category(label, predicted_label)            \n",
    "        self.metrics[0].update_state(loss_color)  \n",
    "        self.metrics[1].update_state(loss_category)  \n",
    "        return predicted_color, predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder from https://arxiv.org/pdf/1712.03400.pdf\n",
    "\n",
    "# model\n",
    "\n",
    "# create the whole autoencoder model\n",
    "# (steal from https://towardsdatascience.com/image-colorization-using-convolutional-autoencoders-fdabc1cb1dbe )\n",
    "\n",
    "#encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    #input 1,128,128\n",
    "    self.conv1 = Conv2D(64, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv2 = Conv2D(128, 3, activation='relu', padding='same', strides=2) \n",
    "    self.conv3 = Conv2D(128, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv4 = Conv2D(256, 3, activation='relu', padding='same', strides=2) \n",
    "    self.conv5 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv6 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv7 = Conv2D(512, 3, activation='relu', padding='same', strides=1) \n",
    "    self.conv8 = Conv2D(256, 3, activation='relu', padding='same', strides=1) \n",
    "\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x, training=False):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.conv6(x)\n",
    "    x = self.conv7(x)\n",
    "    x = self.conv8(x)\n",
    "    x = self.flatten(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reshape = Reshape((32, 32, 256))\n",
    "        \n",
    "        self.conv1 = Conv2D(256, 3, activation=\"relu\", padding=\"same\", strides=1)\n",
    "        self.conv2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.upsampling2 = UpSampling2D(2)\n",
    "        self.conv3 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")\n",
    "        self.conv4 = Conv2D(64, 3, activation=\"tanh\", padding=\"same\")\n",
    "        self.upsampling4 = UpSampling2D(2)\n",
    "        self.conv5 = Conv2D(32, 3, activation=\"tanh\", padding=\"same\")\n",
    "        self.conv5 = Conv2D(2, 3, activation=\"tanh\", padding=\"same\")\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsampling2(x)        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)        \n",
    "        x = self.upsampling4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, optimizer, loss_function):\n",
    "    super().__init__()\n",
    "    self.enc = Encoder()\n",
    "    self.dec = Decoder()\n",
    "\n",
    "    self.metrics_list = [\n",
    "      tf.keras.metrics.Mean(name=\"loss\")]\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return self.metrics_list\n",
    "  \n",
    "  def get_encoder(self):\n",
    "    return self.enc\n",
    "   \n",
    "  def get_decoder(self):\n",
    "    return self.dec\n",
    "    \n",
    "  def reset_metrics(self):\n",
    "     for metric in self.metrics:\n",
    "        metric.reset_state()\n",
    "\n",
    "  def call(self, input, training=False):\n",
    "    embedding = self.enc(input)\n",
    "    output = self.dec(embedding)\n",
    "    return output\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    images,  label = data\n",
    "    grey_image, color_image = images\n",
    "    with tf.GradientTape() as tape: \n",
    "      prediction = self(grey_image, training = True)\n",
    "      loss = self.loss_function(color_image, prediction)\n",
    "\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "    self.metrics[0].update_state(loss)  \n",
    "    return gradients\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "    images, label = data\n",
    "    grey_image, color_image = images    \n",
    "    prediction = self(grey_image, training = False)\n",
    "    loss = self.loss_function(color_image, prediction)\n",
    "    self.metrics[0].update_state(loss)\n",
    "    return prediction, color_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(predicted_color, predicted_label, data):\n",
    "    images = data[0]\n",
    "    labels = data[1]\n",
    "    #print(data)\n",
    "    #print(data[0])\n",
    "    \n",
    "    grey_images = images[0]\n",
    "    grey_image = grey_images[0]\n",
    "    color_images = images[1]\n",
    "    color_image = color_images[0]\n",
    "    label = labels[0]\n",
    "    \n",
    "\n",
    "    predicted_color_lab_scaled = tf.clip_by_value(((predicted_color + [0, 0]) * [128, 128]), -128, 127)\n",
    "    color_image_lab_scaled = tf.clip_by_value(((color_image + [0, 0]) * [128, 128]), -128, 127)\n",
    "    greyscale = tf.squeeze((grey_image + [0]) * [100], axis=-1)\n",
    "        \n",
    "    rgb_prediction = lab2rgb([greyscale, predicted_color_lab_scaled[:,:,0], predicted_color_lab_scaled[:,:,1]], channel_axis=0)\n",
    "    rgb_original = lab2rgb([greyscale, color_image_lab_scaled[:,:,0], color_image_lab_scaled[:,:,1]], channel_axis=0)\n",
    "    rgb_prediction = tf.transpose(rgb_prediction, perm=[1,2,0])\n",
    "    rgb_original = tf.transpose(rgb_original, perm=[1,2,0])\n",
    "    \n",
    "    #print(predicted_color_lab_scaled)\n",
    "\n",
    "\n",
    "    #labels\n",
    "\n",
    "    predicted_label = inverse_lookup_table[tf.argmax(predicted_label).numpy()]\n",
    "    true_label = inverse_lookup_table[tf.argmax(label).numpy()]\n",
    "\n",
    "    print(predicted_label, true_label)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (18, 30))\n",
    "    ax[0].imshow(rgb_prediction) \n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('pred: ' + predicted_label)\n",
    "    \n",
    "    ax[1].imshow(rgb_original) \n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('orig: ' + true_label)\n",
    "    plt.imshow(rgb_original)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# log results with tensorboard \n",
    "# save model to be able to reuse it\n",
    "\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        \n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            predicted_color, predicted_label = model.train_step(data)\n",
    "\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        print(\"Loss Category: \", model.metrics[1].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        last_data = None\n",
    "        for data in tqdm(test_ds, position=0, leave=True):\n",
    "            predicted_color, predicted_label = model.test_step(data)\n",
    "            last_data = data\n",
    "\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "            \n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "        print(\"Loss Category: \", model.metrics[1].result().numpy(), \"(Test)\")\n",
    "\n",
    "        if (epoch // 5) == 0:\n",
    "            visualize(predicted_color[0], predicted_label[0], last_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_colorization(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        \n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            predicted_color = model.train_step(data)\n",
    "\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        last_data = None\n",
    "        for data in tqdm(test_ds, position=0, leave=True):\n",
    "            predicted_color = model.test_step(data)\n",
    "            last_data = data\n",
    "\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            \n",
    "        print(\"Loss Color: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "\n",
    "        if (epoch // 10) == 0:\n",
    "            visualize(predicted_color[0], last_data[1][0], last_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_classification(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        \n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            predicted_label = model.train_step(data)\n",
    "\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss Category: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        last_data = None\n",
    "        for data in tqdm(test_ds, position=0, leave=True):\n",
    "            predicted_label = model.test_step(data)\n",
    "            last_data = data\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            \n",
    "        print(\"Loss Category: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "\n",
    "        print(inverse_lookup_table[tf.argmax(predicted_label[0]).numpy()], \n",
    "              inverse_lookup_table[tf.argmax(last_data[1][0]).numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Loss Category:  4.787528 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 26.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875037 (Test)\n",
      "Airedale miniature_poodle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Loss Category:  4.78749 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875104 (Test)\n",
      "Airedale bull_mastiff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "Loss Category:  4.7874703 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875075 (Test)\n",
      "Pembroke bluetick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "Loss Category:  4.787457 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787512 (Test)\n",
      "Airedale Irish_water_spaniel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "Loss Category:  4.787447 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875104 (Test)\n",
      "Saluki West_Highland_white_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n",
      "Loss Category:  4.7874336 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875147 (Test)\n",
      "Saluki flat-coated_retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n",
      "Loss Category:  4.7874246 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875166 (Test)\n",
      "Samoyed African_hunting_dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n",
      "Loss Category:  4.787413 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875204 (Test)\n",
      "Sealyham_terrier French_bulldog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n",
      "Loss Category:  4.787401 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:02<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875237 (Test)\n",
      "Saluki German_shepherd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n",
      "Loss Category:  4.7873907 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787525 (Test)\n",
      "Samoyed malamute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "Loss Category:  4.7873807 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875204 (Test)\n",
      "Airedale Irish_water_spaniel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12\n",
      "Loss Category:  4.787372 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787539 (Test)\n",
      "Maltese_dog wire-haired_fox_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13\n",
      "Loss Category:  4.787362 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875323 (Test)\n",
      "Maltese_dog kelpie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14\n",
      "Loss Category:  4.7873507 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875423 (Test)\n",
      "Maltese_dog Shetland_sheepdog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15\n",
      "Loss Category:  4.787341 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875376 (Test)\n",
      "Maltese_dog Lakeland_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16\n",
      "Loss Category:  4.7873297 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787539 (Test)\n",
      "Maltese_dog wire-haired_fox_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17\n",
      "Loss Category:  4.7873216 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787546 (Test)\n",
      "Maltese_dog schipperke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18\n",
      "Loss Category:  4.787313 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875476 (Test)\n",
      "Maltese_dog Italian_greyhound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19\n",
      "Loss Category:  4.7872996 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787559 (Test)\n",
      "Maltese_dog curly-coated_retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20\n",
      "Loss Category:  4.7872906 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787553 (Test)\n",
      "Maltese_dog Border_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21\n",
      "Loss Category:  4.7872806 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875524 (Test)\n",
      "Maltese_dog Irish_wolfhound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22\n",
      "Loss Category:  4.7872677 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787554 (Test)\n",
      "Maltese_dog collie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23\n",
      "Loss Category:  4.78726 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787563 (Test)\n",
      "Maltese_dog malamute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24\n",
      "Loss Category:  4.78725 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875557 (Test)\n",
      "Maltese_dog briard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25\n",
      "Loss Category:  4.7872415 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875714 (Test)\n",
      "Maltese_dog African_hunting_dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26\n",
      "Loss Category:  4.787234 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875614 (Test)\n",
      "Maltese_dog giant_schnauzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  27\n",
      "Loss Category:  4.78722 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875843 (Test)\n",
      "Maltese_dog standard_poodle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  28\n",
      "Loss Category:  4.7872157 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875767 (Test)\n",
      "Maltese_dog Old_English_sheepdog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29\n",
      "Loss Category:  4.787201 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875776 (Test)\n",
      "Maltese_dog Tibetan_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  30\n",
      "Loss Category:  4.787194 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875752 (Test)\n",
      "Maltese_dog Bernese_mountain_dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  31\n",
      "Loss Category:  4.7871814 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.787582 (Test)\n",
      "Maltese_dog Blenheim_spaniel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  32\n",
      "Loss Category:  4.78717 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875943 (Test)\n",
      "Maltese_dog Kerry_blue_terrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:34<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  33\n",
      "Loss Category:  4.7871647 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 29.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875886 (Test)\n",
      "Maltese_dog otterhound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:35<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  34\n",
      "Loss Category:  4.787154 (Train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Category:  4.7875953 (Test)\n",
      "Maltese_dog Irish_wolfhound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 13/282 [00:01<00:34,  7.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[529], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m test_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(test_log_path)\n\u001b[1;32m     19\u001b[0m \u001b[39m#training_loop_colorization(only_colorization, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m training_loop_classification(only_classification, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)\n",
      "Cell \u001b[0;32mIn[528], line 7\u001b[0m, in \u001b[0;36mtraining_loop_classification\u001b[0;34m(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     predicted_label \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     11\u001b[0m     tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname, model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adadelta()\n",
    "loss_function_color = tf.keras.losses.MeanSquaredError()\n",
    "loss_function_category = tf.keras.losses.CategoricalCrossentropy()\n",
    "#autoencoder = Autoencoder(optimizer=optimizer, loss_function=loss_function)\n",
    "\n",
    "model= Model(optimizer=optimizer, loss_function_color=loss_function_color, loss_function_category=loss_function_category)\n",
    "only_colorization = Only_Colorization_Model(optimizer=optimizer, loss_function_color=loss_function_color)\n",
    "only_classification = Only_Classification_Model(optimizer=optimizer, loss_function_category=loss_function_category)\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"models/{current_time}\"\n",
    "train_log_path = f\"logs/{current_time}/train\"\n",
    "test_log_path = f\"logs/{current_time}/test\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "#training_loop_colorization(only_colorization, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)\n",
    "training_loop_classification(only_classification, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)\n",
    "#training_loop(model, train_dataset, test_dataset, epochs, train_summary_writer, test_summary_writer, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-875463dc9ff1ba47\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-875463dc9ff1ba47\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be5498235a11125f8b9cbeecb800fcd2f42e8279b58f495bdc82a203ff5bcb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
